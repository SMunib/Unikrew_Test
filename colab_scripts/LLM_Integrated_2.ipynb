{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtrQLkvICRFt",
        "outputId": "75a35426-d0d0-4170-b5e9-c216b005beb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVI9PCvuF3jI",
        "outputId": "70d3209f-ad5f-43b9-af06-93c092e343d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-4uu6en1p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-4uu6en1p\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit a1ce2f956a1d2212ad672e3c47d53405c2fe4312\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.0)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Collecting pytokens>=0.1.10 (from black->detectron2==0.6)\n",
            "  Downloading pytokens-0.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytokens-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=6733309 sha256=0bba7c3c214d60aa6d54ab0da54b22ffd2af618dfa921042228bd7d463f70a76\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-px_hux1j/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=f0f0d66bf466abfa8ec9747791e48ca3889dd9a65c2c852619a3ae9be1a1288d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-25.9.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 pytokens-0.2.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KphHfy2GfL3S",
        "outputId": "862b9590-2237-4114-f99f-10623d3146b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.2)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.10.4)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.3)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.2 ninja-1.13.0 pyclipper-1.3.0.post6 python-bidi-0.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp0zb8H3HqGW",
        "outputId": "adda73f1-f01e-4e02-f6e1-4d4a681360d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self,txt_path,img_path,box_path,processor,label2id,img_dir):\n",
        "        self.words,self.labels,self.bboxes,self.images = self._load_data(txt_path,img_path,box_path,img_dir)\n",
        "        self.processor = processor\n",
        "        self.label2id = label2id\n",
        "\n",
        "    def _load_data(self,txt_path,img_path,box_path,img_dir):\n",
        "        words_per_doc,labels_per_doc,bboxes_per_doc,images = [],[],[],[]\n",
        "        with open(txt_path,'r',encoding='utf-8') as f_text,\\\n",
        "             open(img_path,'r',encoding='utf-8') as f_img,\\\n",
        "             open(box_path,'r',encoding='utf-8') as f_box:\n",
        "\n",
        "            words,labels,boxes = [],[],[]\n",
        "            curr_file_name = None\n",
        "            img = None\n",
        "\n",
        "            for (line_text,line_img,line_box) in zip(f_text,f_img,f_box):\n",
        "                if line_text.strip() == \"\":\n",
        "                    if words:\n",
        "                        words_per_doc.append(words)\n",
        "                        labels_per_doc.append(labels)\n",
        "                        bboxes_per_doc.append(boxes)\n",
        "                        images.append(img)\n",
        "                        words,labels,boxes = [],[],[]\n",
        "                    continue\n",
        "\n",
        "                word,label = line_text.strip().split(\"\\t\")\n",
        "                word_box = list(map(int, line_box.strip().split(\"\\t\")[1].split()))\n",
        "                img_info = line_img.strip().split(\"\\t\")\n",
        "                filename = img_info[-1]\n",
        "\n",
        "                if curr_file_name != filename:\n",
        "                    curr_file_name = filename\n",
        "                    img = Image.open(os.path.join(img_dir,filename)).convert(\"RGB\")\n",
        "\n",
        "                words.append(word)\n",
        "                labels.append(label)\n",
        "                boxes.append(word_box)\n",
        "\n",
        "        print(\"Dataset loaded successfully\")\n",
        "        return words_per_doc,labels_per_doc,bboxes_per_doc,images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        encoding = self.processor(\n",
        "            self.images[idx],\n",
        "            self.words[idx],\n",
        "            boxes=self.bboxes[idx],\n",
        "            word_labels=[self.label2id[label] for label in self.labels[idx]],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {k:v.squeeze(0) for k,v in encoding.items()}"
      ],
      "metadata": {
        "id": "QOA-wgpUCqzB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LayoutLMv2ForTokenClassification\n",
        "import torch\n",
        "\n",
        "class LayoutLM_Model:\n",
        "    def __init__(self, model_path,num_labels,id2label,label2id,processor,dataset):\n",
        "        print(\"Initializing LayoutLM_Model....\")\n",
        "        self.model_path = model_path\n",
        "        self.num_labels = num_labels\n",
        "        self.id2label = id2label\n",
        "        self.label2id = label2id\n",
        "        self.processor = processor\n",
        "        self.dataset = dataset\n",
        "        self.device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        self.model = LayoutLMv2ForTokenClassification.from_pretrained(\n",
        "            self.model_path,\n",
        "            num_labels=self.num_labels,\n",
        "            id2label=self.id2label,\n",
        "            label2id=self.label2id\n",
        "        ).to(self.device)\n",
        "        self.model.eval()\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "    def model_infer(self,sample_idx):\n",
        "        \"\"\"\n",
        "        Run inference on a single sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            model: Trained LayoutLM model\n",
        "            processor: LayoutLM processor\n",
        "            dataset: Dataset object containing images, words, and bboxes\n",
        "            sample_idx: Index of the sample to test (e.g., 0, 1, 2...)\n",
        "        \"\"\"\n",
        "        # Set model to evaluation mode\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model is not loaded\")\n",
        "        # Get the selected sample\n",
        "        sample = self.dataset[sample_idx]\n",
        "        # Prepare input (exclude labels)\n",
        "        inputs = {k: v.unsqueeze(0).to(self.device) for k, v in sample.items() if k != \"labels\"}\n",
        "        # Run inference\n",
        "        with torch.inference_mode():\n",
        "            outputs = self.model(**inputs)\n",
        "            preds = outputs.logits.argmax(-1).squeeze().cpu().numpy()\n",
        "\n",
        "        # Recreate encoding for alignment\n",
        "        encoding = self.processor(\n",
        "            self.dataset.images[sample_idx],\n",
        "            self.dataset.words[sample_idx],\n",
        "            boxes=self.dataset.bboxes[sample_idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Initialize expected fields\n",
        "        results = {\"company\": \"\", \"address\": \"\", \"total\": \"\", \"date\": \"\"}\n",
        "\n",
        "        # Track seen words to avoid duplicates\n",
        "        seen = {k: set() for k in results.keys()}\n",
        "\n",
        "        # Decode predictions\n",
        "        for idx, label_id in enumerate(preds):\n",
        "            word_idx = encoding.word_ids(batch_index=0)[idx]\n",
        "            if word_idx is None:\n",
        "                continue  # Skip special/padding tokens\n",
        "\n",
        "            label = self.id2label[label_id]\n",
        "            if label != \"O\":\n",
        "                key = label.replace(\"S-\", \"\").lower()\n",
        "                if key in results:\n",
        "                    word = self.dataset.words[sample_idx][word_idx]\n",
        "                    if word not in seen[key]:\n",
        "                        results[key] += \" \" + word\n",
        "                        seen[key].add(word)\n",
        "\n",
        "        # Clean whitespace\n",
        "        results = {k: v.strip() for k, v in results.items()}\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "ca-ZRJAOCrc8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import json\n",
        "import os\n",
        "\n",
        "class DocumentReasoningAgent:\n",
        "\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-2.0-flash\"):\n",
        "        os.environ['GEMINI_API_KEY'] = api_key\n",
        "        genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    def build_prompt(self, extracted_json: dict, ocr_text: str):\n",
        "        prompt = f\"\"\"\n",
        "        You are a document reasoning agent, specialized in receipts.\n",
        "        You will be given:\n",
        "        1. Extracted structured json from a receipt.\n",
        "        2. The OCR text of the receipt.\n",
        "\n",
        "        Tasks:\n",
        "        - Detect any missing or inconsistent fields/data in the structured json.\n",
        "        - Identify possible vendor names from context if extraction failed or missing from json.\n",
        "        - Correct any obvious inconsistencies (e.g date formats, totals)\n",
        "        - Add an \"agent_comment\" summarizing what u fixed or inferred.\n",
        "        - Respond ONLY in JSON format with these keys:\n",
        "\n",
        "        {{\n",
        "            \"company\": \"...\",\n",
        "            \"date\": \"...\",\n",
        "            \"address\": \"...\",\n",
        "            \"total\": \"...\",\n",
        "            \"agent_comment\": \"...\"\n",
        "        }}\n",
        "\n",
        "        Example Input:\n",
        "        {{\n",
        "            \"company\": \"\",\n",
        "            \"date\": \"2020-10-02\",\n",
        "            \"address\": \"xyz street\",\n",
        "            \"total\": \"\"\n",
        "        }}\n",
        "\n",
        "        Example OCR Text:\n",
        "        \"McDonald's\\n02/10/2020\\nShop 12, Main Street\\nTotal: 15.90\"\n",
        "\n",
        "        Example Output:\n",
        "        {{\n",
        "            \"company\": \"McDonald's\",\n",
        "            \"date\": \"02/10/2020\",\n",
        "            \"address\": \"Shop 12, Main Street\",\n",
        "            \"total\": \"15.90\",\n",
        "            \"agent_comment\": \"The company name and total were inferred from the OCR text.\"\n",
        "        }}\n",
        "\n",
        "        Extracted JSON: {json.dumps(extracted_json,indent=2)}\n",
        "        OCR Text: {ocr_text}\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def build_rejection_prompt(self, reason: str, metrics: dict):\n",
        "        prompt = f\"\"\"\n",
        "        You are a document validation agent.\n",
        "\n",
        "        The provided document image has been flagged as potentially unreadable or invalid\n",
        "        due to image quality issues such as watermarks, obstructions, or black overlays.\n",
        "\n",
        "        Metrics from the vision detector:\n",
        "        {json.dumps(metrics, indent=2)}\n",
        "\n",
        "        Task:\n",
        "        - Review the metrics and reasoning summary.\n",
        "        - Return a JSON response explaining clearly *why* the document was rejected.\n",
        "\n",
        "        Respond ONLY in JSON format:\n",
        "        {{\n",
        "            \"status\": \"rejected\",\n",
        "            \"reason\": \"{reason}\",\n",
        "            \"agent_comment\": \"...\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def _generate_json_response(self, prompt: str):\n",
        "        response = self.model.generate_content(prompt)\n",
        "        raw_output = response.candidates[0].content.parts[0].text.strip()\n",
        "\n",
        "        if raw_output.startswith(\"```\"):\n",
        "            raw_output = raw_output.strip(\"`\")\n",
        "            raw_output = raw_output.replace(\"json\", \"\").strip()\n",
        "\n",
        "        try:\n",
        "            result = json.loads(raw_output)\n",
        "        except json.JSONDecodeError:\n",
        "            result = {\"error\": \"Invalid JSON response\", \"raw_output\": raw_output}\n",
        "        return result\n",
        "\n",
        "    def infer(self, extracted_json: dict, ocr_text: str):\n",
        "        prompt = self.build_prompt(extracted_json, ocr_text)\n",
        "        result = self._generate_json_response(prompt)\n",
        "        if isinstance(result, dict) and \"address\" in result and isinstance(result[\"address\"], str):\n",
        "            result[\"address\"] = result[\"address\"].replace(\"\\n\", \" \").strip()\n",
        "        return result\n",
        "\n",
        "    def infer_rejection(self, reason: str, metrics: dict):\n",
        "        prompt = self.build_rejection_prompt(reason, metrics)\n",
        "        return self._generate_json_response(prompt)\n"
      ],
      "metadata": {
        "id": "huOmmSmcICZ-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class WatermarkDetector:\n",
        "    def __init__(self, dark_threshold=30, dark_ratio_threshold=0.3):\n",
        "        self.dark_threshold = dark_threshold\n",
        "        self.dark_ratio_threshold = dark_ratio_threshold\n",
        "\n",
        "    def is_obscured(self, img):\n",
        "        if img is None:\n",
        "            return False, {\"error\": \"image not provided or invalid\"}\n",
        "\n",
        "        # ✅ Corrected: proper check for PIL Image\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        # Convert to grayscale if needed\n",
        "        if len(img.shape) == 3:\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = img\n",
        "\n",
        "        # Normalize and compute darkness ratio\n",
        "        dark_pixels = np.sum(gray < self.dark_threshold)\n",
        "        total_pixels = gray.size\n",
        "        dark_ratio = dark_pixels / total_pixels\n",
        "\n",
        "        # Basic threshold flag\n",
        "        flagged = dark_ratio > self.dark_ratio_threshold\n",
        "\n",
        "        # Optional: Contour check for large dark rectangles\n",
        "        _, thresh = cv2.threshold(gray, self.dark_threshold, 255, cv2.THRESH_BINARY_INV)\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        large_rects = [cv2.contourArea(c) for c in contours if cv2.contourArea(c) > 5000]\n",
        "        if len(large_rects) > 0:\n",
        "            flagged = True\n",
        "\n",
        "        return flagged, {\"dark_ratio\": dark_ratio, \"large_rects\": len(large_rects)}\n"
      ],
      "metadata": {
        "id": "hD-J9J3q_bBw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class ReceiptOCR:\n",
        "    def __init__(self, lang_list=['en'], gpu=True):\n",
        "        self.reader = easyocr.Reader(lang_list, gpu=gpu)\n",
        "\n",
        "    def extract_text(self, image):\n",
        "        \"\"\"\n",
        "        Extracts plain text from a receipt image (PIL or NumPy) using EasyOCR.\n",
        "        Returns a dictionary ready to feed into an LLM alongside structured JSON.\n",
        "        \"\"\"\n",
        "        # Ensure image is in a NumPy format (EasyOCR expects NumPy array)\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Run OCR\n",
        "        results = self.reader.readtext(image, detail=1)\n",
        "\n",
        "        # Combine text lines in reading order\n",
        "        text_lines = [res[1] for res in results]\n",
        "        full_text = \"\\n\".join(text_lines).strip()\n",
        "\n",
        "        # Prepare structured output\n",
        "        data = {\n",
        "            \"ocr_text\": full_text,\n",
        "            \"lines\": text_lines,\n",
        "        }\n",
        "\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "Iwg77Zd-cVpy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LayoutLMv2Processor\n",
        "\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"S-COMPANY\": 1,\n",
        "    \"S-DATE\": 2,\n",
        "    \"S-ADDRESS\": 3,\n",
        "    \"S-TOTAL\": 4,\n",
        "}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\",apply_ocr=False)\n",
        "\n",
        "test_dataset = ImageDataset(\n",
        "    txt_path=\"/content/drive/MyDrive/UnikrewTest/processed_data/test/test.txt\",\n",
        "    img_path=\"/content/drive/MyDrive/UnikrewTest/processed_data/test/test_img.txt\",\n",
        "    box_path=\"/content/drive/MyDrive/UnikrewTest/processed_data/test/test_box.txt\",\n",
        "    processor=processor,\n",
        "    label2id=label2id,\n",
        "    img_dir=\"/content/drive/MyDrive/UnikrewTest/dataset/test/img\"\n",
        ")\n",
        "\n",
        "inference_model = LayoutLM_Model(\n",
        "    model_path=\"/content/drive/MyDrive/UnikrewTest/modules/model\",\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    processor=processor,\n",
        "    dataset=test_dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuysaXZSFvZD",
        "outputId": "5cbf51d5-f235-4fdf-fc89-3c6d5f1f294b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully\n",
            "Initializing LayoutLM_Model....\n",
            "Model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "receipt_ocr = ReceiptOCR()\n",
        "detector = WatermarkDetector()"
      ],
      "metadata": {
        "id": "TsebM3US_SSp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "sample = test_dataset[idx]\n",
        "sample_img = test_dataset.images[idx]"
      ],
      "metadata": {
        "id": "XwlRX_ZiB9Ib"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DocumentReasoningAgent(api_key=\"AIzaSyCh2vElbkO3E6AwX-1kzkuK6noqr2uueUo\")\n",
        "flagged,metrics = detector.is_obscured(sample_img)\n",
        "if flagged:\n",
        "  result = agent.infer_rejection(reason=\"Detected possible watermark or dark obstructions over text regions.\",metrics=metrics)\n",
        "else:\n",
        "  extracted_json = inference_model.model_infer(idx)\n",
        "  ocr_text = receipt_ocr.extract_text(sample_img)\n",
        "  result = agent.infer(extracted_json,ocr_text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XR6tdOa1iCZY",
        "outputId": "47788aaa-129c-4246-d435-89760fa57836"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'company': 'OJC MARKETING SDN BHD', 'date': '15/01/2019', 'address': 'NO 2 & 4 JALAN BAYU 4_, BANDAR SERI ALAM, 81750 MASAI; JOHOR', 'total': '193.00', 'agent_comment': 'The company name and address were inferred from the OCR text. The total and date were already present in the extracted JSON and matched the OCR text.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAG5YxJEa50w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}